{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Audio Processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pyAudioAnalysis\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import MidTermFeatures\n",
    "\n",
    "# Numerical Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# System utilities\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# will be used to save the model to be used to make future predictions\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion (02): Encoded as:\n",
    "# 01 = neutral\n",
    "# 02 = calm\n",
    "# 03 = happy\n",
    "# 04 = sad\n",
    "# 05 = angry\n",
    "# 06 = fearful\n",
    "# 07 = disgust\n",
    "# 08 = surprised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     file_path  emotion  actor\n",
      "0   data/audio/actor1/03-01-06-01-02-02-02.wav        6      2\n",
      "1   data/audio/actor1/03-01-05-01-02-01-16.wav        5     16\n",
      "2   data/audio/actor1/03-01-08-01-01-01-14.wav        8     14\n",
      "3   data/audio/actor1/03-01-06-01-02-02-16.wav        6     16\n",
      "4   data/audio/actor1/03-01-05-01-02-01-02.wav        5      2\n",
      "5   data/audio/actor1/03-01-01-01-02-02-06.wav        1      6\n",
      "6   data/audio/actor1/03-01-02-01-02-01-12.wav        2     12\n",
      "7   data/audio/actor1/03-01-01-01-02-02-12.wav        1     12\n",
      "8   data/audio/actor1/03-01-02-01-02-01-06.wav        2      6\n",
      "9   data/audio/actor1/03-01-02-02-01-01-06.wav        2      6\n",
      "10  data/audio/actor1/03-01-02-02-01-01-12.wav        2     12\n",
      "11  data/audio/actor1/03-01-06-02-01-02-16.wav        6     16\n",
      "12  data/audio/actor1/03-01-05-02-01-01-02.wav        5      2\n",
      "13  data/audio/actor1/03-01-08-02-02-01-14.wav        8     14\n",
      "14  data/audio/actor1/03-01-06-02-01-02-02.wav        6      2\n",
      "15  data/audio/actor1/03-01-05-02-01-01-16.wav        5     16\n",
      "16  data/audio/actor1/03-01-05-01-01-01-22.wav        5     22\n",
      "17  data/audio/actor1/03-01-08-01-02-01-20.wav        8     20\n",
      "18  data/audio/actor1/03-01-06-01-01-02-22.wav        6     22\n",
      "19  data/audio/actor1/03-01-08-01-02-01-08.wav        8      8\n"
     ]
    }
   ],
   "source": [
    "# Define data path\n",
    "data_path = \"data/audio/actor1\"\n",
    "files = []\n",
    "\n",
    "# Iterate through each file and extract features\n",
    "for file in os.listdir(data_path):\n",
    "    if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        # Parse labels from filename\n",
    "        parts = file.split(\"-\")\n",
    "        emotion = int(parts[2])  # Extract emotion from filename\n",
    "        actor = int(parts[-1].split(\".\")[0])  # Extract actor ID\n",
    "        files.append({\"file_path\": file_path, \"emotion\": emotion, \"actor\": actor})\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame(files)\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give you the exact type of emotion the audio is displaying\n",
    "\n",
    "emotion_dict = {\n",
    "    1: \"neutral\",\n",
    "    2: \"calm\",\n",
    "    3: \"happy\",\n",
    "    4: \"sad\",\n",
    "    5: \"angry\",\n",
    "    6: \"fearful\",\n",
    "    7: \"disgust\",\n",
    "    8: \"surprised\"\n",
    "}\n",
    "\n",
    "data[\"emotion_label\"] = data[\"emotion\"].map(emotion_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    file_path  emotion  actor emotion_label\n",
      "0  data/audio/actor1/03-01-06-01-02-02-02.wav        6      2       fearful\n",
      "1  data/audio/actor1/03-01-05-01-02-01-16.wav        5     16         angry\n",
      "2  data/audio/actor1/03-01-08-01-01-01-14.wav        8     14     surprised\n",
      "3  data/audio/actor1/03-01-06-01-02-02-16.wav        6     16       fearful\n",
      "4  data/audio/actor1/03-01-05-01-02-01-02.wav        5      2         angry\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, duration=2.5, offset=0.5)\n",
    "\n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = mfccs.mean(axis=1)\n",
    "    mfccs_std = mfccs.std(axis=1)\n",
    "\n",
    "    # Extract Chroma Features\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_mean = chroma.mean(axis=1)\n",
    "\n",
    "    # Extract Mel Spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_mean = mel.mean(axis=1)\n",
    "\n",
    "    # Spectral Contrast\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    contrast_mean = contrast.mean(axis=1)\n",
    "\n",
    "    # Tonnetz\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    tonnetz_mean = tonnetz.mean(axis=1)\n",
    "\n",
    "    # Zero-Crossing Rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    zcr_mean = zcr.mean()\n",
    "\n",
    "    # Spectral Centroid\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    centroid_mean = centroid.mean()\n",
    "\n",
    "    # Spectral Bandwidth\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    bandwidth_mean = bandwidth.mean()\n",
    "\n",
    "    # Spectral Roll-off\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    rolloff_mean = rolloff.mean()\n",
    "\n",
    "    # Combine all features into a single array\n",
    "    features = np.hstack([\n",
    "        mfccs_mean, mfccs_std, chroma_mean, mel_mean, \n",
    "        contrast_mean, tonnetz_mean, zcr_mean, \n",
    "        centroid_mean, bandwidth_mean, rolloff_mean\n",
    "    ])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['file_path', 'emotion', 'actor', 'emotion_label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare features and labels\n",
    "data[\"features\"] = data[\"file_path\"].apply(lambda x: extract_features(x) if x is not None else np.zeros(30))\n",
    "\n",
    "X = np.array(data[\"features\"].tolist())\n",
    "y = data[\"emotion\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.50      0.29      0.37        17\n",
      "        calm       0.53      0.96      0.68        28\n",
      "       happy       0.49      0.49      0.49        37\n",
      "         sad       0.64      0.51      0.57        45\n",
      "       angry       0.89      0.62      0.73        50\n",
      "     fearful       0.59      0.48      0.53        33\n",
      "     disgust       0.38      0.55      0.44        33\n",
      "   surprised       0.66      0.64      0.65        45\n",
      "\n",
      "    accuracy                           0.58       288\n",
      "   macro avg       0.58      0.57      0.56       288\n",
      "weighted avg       0.61      0.58      0.58       288\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emotion_model']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=list(emotion_dict.values())))\n",
    "\n",
    "joblib.dump(clf, 'emotion_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Model Used Chat to get a simple paragraph below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data evaluates a classification model's performance in predicting emotions, showing metrics like precision, recall, and F1-score for each class (e.g., *happy, sad, angry*). Accuracy (0.61) indicates the model correctly predicted 61% of the 288 samples. Macro averages (precision: 0.60, recall: 0.60, F1: 0.59) show the unweighted mean performance across all classes, treating each equally. Weighted averages (precision: 0.62, recall: 0.61, F1: 0.61) account for class sizes, giving more weight to larger classes. The model performs best on \"angry\" (F1: 0.77) and struggles with \"neutral\" and \"disgust.\" Overall, the model has moderate performance, with room for improvement, particularly in underrepresented classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
